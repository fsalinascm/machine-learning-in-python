{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in scope\n",
    "- what is clustering?\n",
    "- clustering algorithms\n",
    "    - k-means\n",
    "    - hierarchical clustering\n",
    "        - affinity propagation\n",
    "- clustering word embeddings\n",
    "    - how to get insight from clusters of word embeddings\n",
    "- labeling clusters using a hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's not in scope?\n",
    "- any other clustering algorithm\n",
    "- any other unsupervised learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Clustering?\n",
    "\n",
    "Remember how we had feature vectors (or word embeddings) that lived in n-dimensional space? Clustering is an [unsupervised learning algorithm](https://en.wikipedia.org/wiki/Unsupervised_learning) that puts similar feature vectors into the same group or cluster. \n",
    "\n",
    "![hierarchical clustering](figures/hierarchical-clustering.gif)\n",
    "\n",
    "## When should I use clustering?\n",
    "\n",
    "If you're wondering \"what don't I know about this data set?\", clustering will give some insight. \n",
    "\n",
    "Clustering can also be used as a feature extraction piece in the predictive modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's load in the FastText model from before so we're ready to cluster them\n",
    "import os\n",
    "\n",
    "model_file = os.path.join('models', 'phrased_got_ft.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in all of these clustering trials, we'll need a way to link\n",
    "# the word vector from FastText to the word itself. \n",
    "\n",
    "\"\"\"\n",
    "---------------------------------------------------------------\n",
    "Objective: Keep track of your words and vectors in a smart way\n",
    "---------------------------------------------------------------\n",
    "each clustering algorithm will need the input as an iterable of \n",
    "vectors (and nothing else)\n",
    "\n",
    "it'll return a cluster model object, which has an attribute model.labels_\n",
    "\n",
    "These labels will be an index of the cluster this vector fell into\n",
    "\n",
    "But we want to know what _word_ fell into each cluster\n",
    "\n",
    "So we need a map to know which vector corrresponds to each word\n",
    "\n",
    "You might want a few jupyter notebook cells to explore \n",
    "what the FastText model has (maybe a vocab attribute?)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to do this a few times, so let's make a function\n",
    "\n",
    "def get_vectors_and_words(model):\n",
    "    pass\n",
    "\n",
    "vectors, words = get_vectors_and_words(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Algorithms\n",
    "\n",
    "### k-means\n",
    "![k-means](figures/K-means-convergence.gif)\n",
    "\n",
    "As an input, the k-means algorithm requires the number of clusters present `k`, and a maximum number of iterations `i`.\n",
    "\n",
    "This algorithms consists of alternating two steps `i` times\n",
    "1. assignment step: Assign each observation to the cluster whose mean has the least squared Euclidean distance, this is intuitively the \"nearest\" mean.\n",
    "2. update step: Calculate the new means (centroids) of the observations in the new clusters. Do this by averaging the vectors for all instances in a given cluster\n",
    "\n",
    "Quiz: What's the run time for k-means for `n` data points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's implement k-means on our word data\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the attribute of the KMeans model model.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-----------------------------------------------------------------\n",
    "Objective: find a way to view words that are in the same cluster\n",
    "-----------------------------------------------------------------\n",
    "Right now we have the words ['these', 'are', 'unique', 'vocab', 'items', ...]\n",
    "and the labels [0, 1, 1, 0, 2, ...]\n",
    "These labels don't mean anything except that all the 1s go together, all the 2s, and so on\n",
    "\n",
    "Figure out a way to hold this information so you can see which words are in which clusters\n",
    "'''\n",
    "\n",
    "def get_clusters(words, cluster_model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first few clusters by printing them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did K-means do?\n",
    "\n",
    "How do the clusters look? Do they make sense? \n",
    "\n",
    "How do we know that our number of clusters guess was right?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "\n",
    "[Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) is a method of cluster analysis which seeks to build a hierarchy of clusters. There are two ways to do this\n",
    "\n",
    "1. Agglomerative: bottom-up. Each observation starts in its own cluster and pairs of clusters are merged as we work up the hierarchy\n",
    "2. Divisive: top-down. All observations start in one cluster, and splits are performed recursively as we move down the hierarchy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affinity Propagation - a type of Hierarchical Clustering\n",
    "![affinity propagation](figures/affinity-propagation.gif)\n",
    "\n",
    "gif taken from [NIPS 2017 paper video](https://www.youtube.com/watch?v=1IOEFNGPNJc)\n",
    "\n",
    "From [sklearn's documentation](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation):\n",
    ">AffinityPropagation creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given.\n",
    "\n",
    "One of the major benefits of affinity propagation is that it determines the number of clusters automatically. \n",
    "\n",
    "One of the downfalls of affinity propagation is the run time: `O(in^2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more parameter's that come in sklearn, check [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "---------------------------------------\n",
    "objective: cluster the word embeddings\n",
    "---------------------------------------\n",
    "we're going to use sklearn's affinity propagation \n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many clusters were generated? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the clusters by printing the first 20\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty neat! We get some clusters, and we can see how the words are put in the same groups. \n",
    "\n",
    "Is there anything kind of annoying? Too many one-word clusters? Clusters with too many words? Let's remove them to find some more insightful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_insightful_clusters(clusters, min_len=3, max_len=25):\n",
    "    pass\n",
    "\n",
    "get_insightful_clusters(clusters)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These clusters are okay, but there's definitely a lot that could be improved. \n",
    "\n",
    "Quiz: What would make these clusters better?\n",
    "- more data\n",
    "- longer convergence iter\n",
    "- less weight on character parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Wait a second, what do you mean by better?_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do I compare two clustering results?\n",
    "\n",
    "A metric exists to do this: the [silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)#:~:targetText=The%20silhouette%20value%20is%20a,poorly%20matched%20to%20neighboring%20clusters.). \n",
    "\n",
    "> The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "\n",
    "From [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)\n",
    "> The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a, b). To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of. \n",
    "\n",
    "Sure was clear, eh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a better definition of $a$ and $b$. \n",
    "\n",
    "![intra-cluster distance](figures/silhouette-a.png)\n",
    "\n",
    "So for each point $i$ in a cluster, we define $a(i)$ to be the average distance between $i$ and every other point in the cluster. This is a measure of how well $i$ fits in its cluster. \n",
    "\n",
    "What kind of value (smaller or larger) for $a(i)$ indicates a good fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mean nearest-cluster distance](figures/silhouette-b.png)\n",
    "\n",
    "We find the next-best fitting (or _neighboring_) cluster for point $i$, and measure the average distance from $i$ to all points in that next-best fitting cluster. \n",
    "\n",
    "What does a high value for $b(i)$ indicate? A low one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually to the funciton for the silhouette score...\n",
    "\n",
    "![silhouette-score-def](figures/silhouette-def.png)\n",
    "\n",
    "Which can also be written as a piecewise function that's easier to analyze\n",
    "\n",
    "![silhouette-rewritten](figures/silhouette-rewritten.png)\n",
    "\n",
    "How do $a(i)$ and $b(i)$ compare in each of these situations? \n",
    "\n",
    "Does this make the silhouette score bounded or unbounded?\n",
    "\n",
    "Is a higher or lower silhouette score better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a visualization of what's really going on.\n",
    "\n",
    "![silhouette visualized](figures/silhouette-visualized.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we care about more than a single point in the clustering results, we average over all points to get teh silhouete score for a data set.\n",
    "\n",
    "![Silhouette scores](figures/silhouette_analysis.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03934189"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "# sklearn computes the mean silhouette score for all samples\n",
    "silhouette_score(vectors, af.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this with the unphrased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the unphrased model\n",
    "unphrased_model_path = os.path.join('models', 'got_ft.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the silhouette score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which model was better?\n",
    "\n",
    "Objectively: Which model had the better silhouette score? \n",
    "\n",
    "Subjectively: Let's check out the clusters too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first 20 clusters from the unphrased model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, seems like worse clusters without the phrased data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Clusters Using a Hierarchy\n",
    "\n",
    "Powerpoint presentation [here](https://callminer-my.sharepoint.com/:p:/p/kirsten_stallings/Ef_H9hSc15FPnujGQDGVdp4B6zCQGJZSpjgCs_ZfSBChxQ?e=d6zbhY). \n",
    "\n",
    "These clusters are insightful, but there's a lot of them to dig through. It's hard to know where to start. \n",
    "\n",
    "If each cluster had a _label_, we could sort on that label and get things closer together, focusing on the clusters we want to refine first. \n",
    "\n",
    "In order to label these clusters, we need a resource to know which words tend to get this label. This is the _hierarchy_, a tree of terms. \n",
    "\n",
    "Each node cruciall has\n",
    "- a name or label, like `root-feeling-negative_sentiment-angry`\n",
    "\n",
    "and each node _might_ have \n",
    "- children -  child of `root-feeling-negative_sentiment-angry` might be `root-feeling-negative_sentiment-angry-irate`\n",
    "- parents - the parent of `root-feeling-negative_sentiment-angry` is `root-feeling-negative_sentiment`\n",
    "- words that are associated with this label, like `upset, frustrated, in_denial, angry`\n",
    "\n",
    "Each child of that node contains concepts that are more specific. \n",
    "\n",
    "For instance, \n",
    "> dog -> herders -> shetland sheepdog\n",
    "\n",
    "Given a cluster and a word embedding model, the _labeler_ moves through the hierarchy to determine the best label for this unseen cluster.\n",
    "\n",
    "![hiearchy and labeler](figures/hierarchy-and-labeler.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hierarchy\n",
    "\n",
    "Here's an example from the hierarchy we created by using clusters from call center data. \n",
    "\n",
    "![hierarchy expanding](figures/hierarchy-expanding.gif)\n",
    "\n",
    "Each of these children of root holds a lot of significance in the call center space. That's the first distinction we make on a word/phrase!\n",
    "\n",
    "These concepts are crucially never client or product specific. Since this will be used across many different clients, we don't want the product name from one to influence another incorrectly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Labeler\n",
    "\n",
    "The labeler moves throught the hierarchy to find the best fitting name for the unseen cluster generated above in the clustering step. \n",
    "\n",
    "First, we find the centroid of the unseen cluster. We average all the word vectors to get the _centroid_, a vector that represents this cluster in space. \n",
    "\n",
    "The labeler is looking to find the closest centroid in the hierarchy to the centroid of the cluster we are currrently looking at. We look at two centroids:\n",
    "\n",
    "1. The vectors for the words in the node and it's subtree\n",
    "2. The vectors for only the words in the node.\n",
    "\n",
    "We use the client's FastText model to calculate the centroids of the hierarchy nodes, effectively projecting the hierarchy into the client's space. \n",
    "\n",
    "Quiz: _Why is it important to use the client's model here?_\n",
    "\n",
    "We take all the word vectors in the subtree rooted at the current node and average them to get the centroid. The serves as the middle location for this node in the the client's vector space. For each child of root, we have a centroid. \n",
    "\n",
    "We also compare with the centroid based on the words at this node _only_. If either of the two of those centroids is closer than the previously-considered-closest centroid, we move to this node in the tree and repeat with its children. \n",
    "\n",
    "When the closest centroid is found (none of the children's centroids are closer than the current node's), we label this cluster with the name of this node in the hierarchy. \n",
    "\n",
    "![moving through the hierarchy](figures/moving-through-the-hierarchy.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can't show you any code for this - we'd have to build a whole new GOT hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of the hierarchy and labeler performing on real call center clusters:\n",
    "\n",
    "![real life examples](figures/labeled-clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- what is clustering?\n",
    "- clustering algorithms\n",
    "    - k-means\n",
    "    - hierarchical clustering\n",
    "        - affinity propagation\n",
    "- clustering word embeddings\n",
    "    - how to get insight from clusters of word embeddings\n",
    "- labeling clusters using a hierarchy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
